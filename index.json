[{"authors":null,"categories":null,"content":"I am Ran Zhu, a graduate candidate studying in Communication and information system in the Department of Internet of Things Engineering, UESTC (Chengdu).\nMy research interests lie in indoor pedestrian positioning and intelligent navigation for autonomous robots, especially about PDR, visual odometry, and visual SLAM. Previously I had been working on human activity recognition using smartphones and Zero velocity detection in ZUPT based on inertial for pedestrian navigation. I design new algorithms (increasing machine and deep learning based) and apply these innovations to solving these issues.\nCurrently, I am investigating how to localize people and objects in environments where technologies like GPS fail, such as underground or indoors. For people positioning, my work is to address the map matching problem of inertial navigation by leveraging architectural constraints and visual information when there is no starting point. For robot navigation, my research aims to build an intelligent robotic system that can more efficiently deal with the traditional SLAM problems by combining traditional methods and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ran-zhu-%E6%9C%B1%E7%84%B6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ran-zhu-%E6%9C%B1%E7%84%B6/","section":"authors","summary":"I am Ran Zhu, a graduate candidate studying in Communication and information system in the Department of Internet of Things Engineering, UESTC (Chengdu).\nMy research interests lie in indoor pedestrian positioning and intelligent navigation for autonomous robots, especially about PDR, visual odometry, and visual SLAM.","tags":null,"title":"Ran Zhu (朱然)","type":"authors"},{"authors":["Ran Zhu, Mingkun Yang, Wang Liu, Rujun Song, Zhuoling Xiao, Bo Yan"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"2af9733828cb92f53c9f2063e306d07d","permalink":"/publication/project4/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/project4/","section":"publication","summary":"To build an intelligent robotic positioning system that can more efficiently deal with the traditional SLAM problems such as camera calibration, monocular scale ambiguity, we explore a novel strategy for performing visual ego-motion estimation based on deep learning. Here, we extend the model into four branches focusing on pixels movement in different directions in the optical flow and then regress the global feature concatenated from the four outputs to obtain F2F motion estimation. In particular, features extracted by each branch have been distilled by using the attention mechanism to refine estimation. Experiments on the KITTI and Malaga benchmark datasets demonstrate that the proposed model outperforms state-of-the-art monocular methods by a large margin and produces competitive results against the classic stereo VO algorithm, which also highlights its promising generalization ability.","tags":["Source Themes"],"title":"DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry","type":"publication"},{"authors":null,"categories":null,"content":"From Unmanned Ground Vehicles (UGVs) to Micro Aerial Vehicles (MAVs), it is essential to know where autonomous robots are and to perceive the surrounding area. Global Positioning System (GPS) provides information about the position of the sensor in the world coordinate. However, a precise self-localization purely relying on the GPS is not sufficient for challenging environments like indoor scenarios and urban canyons. In this situation, a more precise measure or an alternative localization system is required in the real application for autonomous driving.\nThe camera is a small, light-weighted sensor that provides rich information about the environment around the sensing platform. It can recover the ego-motion from image sequences by exploiting the consistency between consecutive frames. Therefore, The concept of Visual Simultaneous Localization And Mapping (V-SLAM) and Visual Odometry (VO) are proposed to solve the well-known problem of positioning, which estimates vehicles' position relative to its start point. As an essential task in robotics and computer vision communities, VO has been widely applied to various applications, ranging from autonomous driving and space exploration to virtual and augmented reality. From the perspective of the camera used, the VO methods consist of two types: stereo VO and monocular VO. This work aims at investigating the monocular VO, for a single camera is cheaper, lighter, and more general than a stereo rig. Especially when the ratio of stereo baseline to depth is minimal, the stereo VO degenerates to the monocular one.\nOver the past thirty years, enormous work has been done to develop an accurate and robust VO system. The traditional VO algorithms can be divided into the feature-based method and the direct method. Feature-based methods typically consist of camera calibration, feature detection, feature matching, outlier rejection (e.g., RANSAC), motion estimation, scale estimation, and optimization (e.g., Bundle Adjustment). Unfortunately, how to detect appropriate features for recovering specific motions remains a challenging problem. Handcrafted feature descriptors such as SIFT, ORB designed for general visual tasks, lack the response to motions. Instead, extra information that is guided by geometric prior such as planar structures and vanishing points, is used for camera pose estimation in specific environments, providing promising performance but limited generalization ability. Unlike feature-based methods, direct methods track the motion of the pixel and obtain pose prediction by minimizing the photometric error, so it is extremely vulnerable to light changes. Moreover, the absolute scale estimation in the traditional monocular VO must use some extra information (e,g., the height of the camera) or prior knowledge.\nThe emerging Deep Learning (DL), a data-driven approach, has yielded impressive achievement in the computer vision. Rather than handcrafted features, DL that has the ability to extract deep features from the plain input, encodes the high-level priors to regress camera poses. Compared with traditional VO, learning-based VO has the advantage of low computation cost and no need for internal camera parameters. A few methods on DL have been proposed for camera motion recovery, such as DeepVO, ESP-VO, SfmLearner, and GeoNet. While achieving promising performances, they do not take into account the different responses of visual cues and the effect of pixels movement in different directions in the input image to the camera motion, thus may output trajectories with large error. For learning-based VO, it should focus more on geometric constraints than the \u0026ldquo;appearance'' information when harnessing Convolutional Neural Networks (CNNs) to extract features. Optical flow, as the representation of the geometric structure, has been proved useful for estimating Frame to Frame (F2F) ego-motion. Therefore, we take the optical flow as input to the proposed model.\nGuided by the previous considerations, we explore a novel strategy for performing visual ego-motion estimation in this work. Here, we extend the network into four branches focusing on pixels movement in different directions in the optical flow and then regress the global feature concatenated from the four outputs to obtain F2F motion estimation. In particular, features extracted by each branch have been distilled by using the attention mechanism to refine estimation. In this project, many quantitative and qualitative experiments in terms of precision, robustness, and computation speed are conducted. The results demonstrate that the proposed model outperforms many current monocular methods and provides a competitive performance against the classic stereo VO. In summary, our key contributions are as follows\u0026amp;#58\n Novel visual perception guiding ego-motion estimation\u0026amp;#58 By considering the four quadrants in optical flow and fusing the distilling module into each branch encoder, the learning-based DeepAVO model pays more attention to the visual cues that are effective for ego-motion estimation. Lightweight VO framework with enhanced tracking performance\u0026amp;#58 The proposed DeepAVO model framework yields more robust and accurate results compared with competing monocular VOs. The F2F VO calculation can be done within 12 ms, making it practical and valuable in real-world applications. Extensive fresh scenes validation\u0026amp;#58 The DeepAVO produces promising pose estimation and maintains high-precision tracking results on the 11-20 sequence of the KITTI dataset and the Malaga dataset. Outstanding improvements in the accuracy and robustness of VO are further demonstrated.  ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"68fe1e900090392e3d2a97574964bc5a","permalink":"/project/project3/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/project3/","section":"project","summary":"Efficient pose estimation with featuredistilling for deep visual odometry.","tags":["Mobile Robotics"],"title":"Visual Odometry","type":"project"},{"authors":null,"categories":null,"content":"Visual simultaneous localization and mapping (vSLAM) that simultaneously recovers camera pose and scene structure from video, as one of the key autonomous positioning and navigation technologies in areas where GPS fails or cannot be covered, is gaining importance in robotic applications such as autonomous cars or unmanned aerial vehicles. Loop Closure Detection (LCD), considered one of the essential parts in the visual SLAM system, is designed to recognize previsited areas by an autonomous mobile robot according to the image information collected by the visual sensors during the moving so also known as visual place recognition. Accurate LCD methods offer precise pose estimation by introducing extra constraints to correct the trajectory drift over time, improving the system performance. However, there are still two common challenges\u0026amp;#58 1) the same place has different appearances at different times due to change of illumination and weather; 2) different scenes look similar for reasons such as sharing common objects. Therefore, an excellent LCD method needs to resolve these two problems to detect more correct loops.\nIn the field of vSLAM, the appearance-based methods treating LCD as an image matching problem compare the similarity between the current image and previous images. If the similarity between them is sufficiently high to exceed a given threshold, we can regard it as a loop closure. As one of the most popular LCD approaches, it can be divided into two crucial steps: feature generation and similarity measurement.\nIn the vSLAM system, image feature extraction forms the basis of a series of tasks, such as keyframe extraction, tracking, positioning, and map construction, which have a decisive influence on the robot’s autonomous positioning. The traditional appearance-based methods mainly follow the visual bag-of-words (BoWs) model, which uses a clustering procedure on a training sample of local features and quantizes the descriptor space into Visual Words (VWs). However, this approach uses hand-crafted traditional features. Most of these features discard certain geometric and structural information, making it difficult to cope with the challenges such as camera motion and illumination changes. Moreover, BoWs relies on specific environments and has poor robustness to different application scenarios. In recent years, deep learning has made significant breakthroughs in the field of computer vision. Much related research demonstrates that the deep features learned by convolutional neural networks (CNNs) can provide more robust image representations in changeable environmental conditions, especially when illumination change and viewpoint variation. Besides, the network models trained for specific tasks can be transferred to other tasks successfully. Some classic pre-training network models’ availability makes it more convenient to complete various feature extraction tasks. Based on the above reasons, researchers began to apply CNNs to LCD. Although good results have been achieved, these works paid little attention to similarity measurement strategies, which also play a crucial role in image matching tasks. Fixed pre-specified distance metrics such as Euclidean distance or cosine similarity are commonly used. Moreover, feature extraction exists independently of similarity measurement, its effectiveness severely restrains the similarity measurement’s effect. For these reasons, further improvement of detection precision is hindered.\nThis project proposes a two-branch end-to-end network called MetricNet that performs LCD based on the adaptive weighted similarity matrix, jointly optimized with feature extraction, to address the issues mentioned earlier. In summary, the key contributions are as follows\u0026amp;#58\n Adaptive feature selection and similarity matrix\u0026amp;#58 By using the channel weighting to remove the irrelevant background information and a weighted similarity matrix to adaptively select spatial information, MetricNet has been proved to be more robust and accurate than competitive models in both theory and practice. An end-to-end LCD framework\u0026amp;#58 this research also proposes a novel LCD framework that encompasses both feature extraction and similarity measurement to extract more effective representation. The learning-based MetricNet can achieve a recall rate up to 47.08% under 100 precision. Extensive multi-dataset validation\u0026amp;#58 MetricNet has proved its excellent performance on three typical open datasets under drastic illumination and seasonal variations. Outstanding improvements in detection accuracy and generalization ability are also further demonstrated.  ","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568505600,"objectID":"e9f2758a8cec6fe970889b4e8ea07727","permalink":"/project/project4/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/project/project4/","section":"project","summary":"A loop closure detection method using adaptive weighted similarity matrix.","tags":["Mobile Robotics"],"title":"Loop Closure Detection","type":"project"},{"authors":["Ying Li, Ran Zhu, Mingkun Yang, Zhuoling Xiao, Yuhan Zhang, Bo Yan"],"categories":null,"content":"","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568505600,"objectID":"6639d817c449fc2a1c77015a76af5142","permalink":"/publication/project5/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/publication/project5/","section":"publication","summary":"Loop Closure Detection (LCD), considered a key optimization part in the visual SLAM system, aims to recognize the places where a mobile robot previously visited. Correct loop closure detection benefits visual SLAM systems a lot because it can significantly reduce the position errors that accumulate over time. It enables the system to build a consistent map of the environment. We introduce the adaptive weighted similarity matrix by combining the feature extraction module and similarity measurement module to focus on changing appearance over time. Experiments on three typical open datasets are conducted to verify the feasibility of the proposed model.","tags":["Source Themes"],"title":"MetricNet: A Loop Closure Detection Method for Appearance Variation using Adaptive Weighted Similarity Matrix","type":"publication"},{"authors":null,"categories":null,"content":"Map-based indoor localization that reconciles the observations with the constraints provided by the maps in order to estimate the most trajectory, has drawn much attention in both civilian and military fields during the last decade. Unlike outdoor localization, it cannot adopt the Global Navigation Satellite System (GNSS) owing to poor penetration. From the perspective of the pre-deployment hardware, indoor localization methods can be divided into two types\u0026amp;#58 deployment-dependent and deployment-independent positioning. A disadvantage to all deployment-dependent positioning methods is the tediousness and delicacy of the deployment process. In a typical deployment processes, a trainer should perform a careful survey of the environment. This includes going to a certain locations, collecting the value of some type of signals and repeating this for probably a large number of points whose locations are known by accurate measurements. This implies several hours or even days of data collection for radio-map of buildings. Most deployment-dependent algorithms need to update databases or maintain equipments. This requires a lot of manpower and material investment. additionally, deployment-dependent positioning will fail in specific scenarios where the facility cannot be deployed in advance. Therefore, this work focuses on map-based deployment-independent indoor positioning.\nAs the key to providing accuracy indoor location, the map can be viewed as a geometric information providing spatial constraints. A floor plan of a building calibrates the movement of a user. For example, people can only enter the room through the door, not through the wall. However, most existing map matching methods rely on the external information of prior site survey or initial positions provided by users, which makes it challenging to meet on-demand localization requirements. Researchers have investigated the possibility of reducing reliance for initial positions.\nInspired by the state-of-the-art researches, we explore a novel strategy for performing deployment-independent indoor positioning in this work. Here, we adopt easily accessible information such as floor plan and simple real-time images captured by smartphones to realize the user positioning by utilizing object recognition, video feature matching, and topological graphic matching all based on deep learning. In particular, topological graphic matching is based on the three-dimensional structure recovered from the floor plan and the spatial information directly extracted from the environmental picture. In summary, the main contributions of this paper are as follows\u0026amp;#58\n On-demand positioning system without initial constraint\u0026amp;#58 We design an indoor positioning system only utilizing map, Inertial Navigation System (INS) and videos to provide ON-DEMAND positioning service. Neither prior site survey nor initial positions from external information is needed. Novel matching method guiding pedestrian positioning\u0026amp;#58 A novel method based on architectural perspective theory is designed to recover three-dimensional structure from floor plan. Besides, a Siamese Network has been employed to match the recovered three-dimensional structure with spatial information extracted from smartphone cameras. Extensive experiments validation\u0026amp;#58 The proposed system demonstrates its performance in terms of tracking accuracy and robustness with a huge number of real-world experiment data in three different experiment scenarios.  ","date":1568419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568419200,"objectID":"80a10a88f419cd9548ab4da835f4ac80","permalink":"/project/project5/","publishdate":"2019-09-14T00:00:00Z","relpermalink":"/project/project5/","section":"project","summary":"Without start points\u0026#58 Indoor positioning using architectural constraints.","tags":["Pedestrian Positioning"],"title":"Visual Inertial Map Matching","type":"project"},{"authors":["Kunxin Li, Mingkun Yang, Ran Zhu, Zhuoling Xiao, Bo Yan"],"categories":null,"content":"","date":1568419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568419200,"objectID":"9db891cf0fb71413d4b1aa26b252ba48","permalink":"/publication/project6/","publishdate":"2019-09-14T00:00:00Z","relpermalink":"/publication/project6/","section":"publication","summary":"This work focuses on map-based deployment-independent indoor positioning. Due to the fact that most existing map matching methods rely on the external information of prior site survey or initial positions provided by users, we explore a novel strategy for performing deployment-independent indoor positioning without considering start points. Here, we adopt easily accessible information such as floor plan and real-time video captured by smartphones to determine the location of the pedestrian for the following tracking.","tags":["Source Themes"],"title":"Visual Inertial Map Matching for Indoor Positioning using Architectural Constraints","type":"publication"},{"authors":["Mingkun Yang, Ran Zhu, Zhuoling Xiao, Bo Yan"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"6252b5ee07edd8fe83e93d49357d6dc0","permalink":"/publication/project3/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/project3/","section":"publication","summary":"Based on the fundamentals of human bipedal motion, Zero Velocity Update (ZUPT) is a pervasive approach in Pedestrian dead reckoning (PDR) to tackle the accumulated error of IMU. The key to ZUPT is precise zero velocity detection that distinguishes the stationary phase from each stride. Besides, due to the complex differences in pedestrian motion patterns, the hope is that the zero velocity detector becomes robust for various individuals. We present a novel approach leveraging deep learning (DL) to detect zero velocity adaptively. Trained by massive foot-mounted IMU data from different individuals, the symmetrical Recurrent Convolutional Neural Network (RCNNs) can effectively learn the gait law because the model takes the information from forward to backward of the undetermined time instant into consideration.","tags":["Source Themes"],"title":"Symmetrical-Net: Adaptive Zero Velocity Detection for ZUPT-Aided Pedestrian Navigation System","type":"publication"},{"authors":null,"categories":null,"content":"Indoor pedestrian tracking cannot count on Global Positioning System (GPS) owing to its poor penetration. Therefore, researchers have proposed various radio frequency localization systems (i.e., Wi-Fi, Bluetooth, ibeacon, etc.) for the indoor environment, but all require installation of infrastructure beforehand. The burden of deploying beacons or landmarks falls on such systems, which is unrealistic in some conditions, especially in the rescue scene.\nPedestrian dead reckoning (PDR) with inertial measurement units (IMU), as the self-contained navigation system without the requirement for ambient conditions, is a justifiable solution for general indoor localization. Nevertheless, the inertial navigation system (INS) calculates the relative displacement of an object that will result in accumulated errors due to the sensors' drift. Stable trajectories could only maintain for a very short term using commercial IMU. To address this issue, foot-mounted IMU with zero velocity update (ZUPT) was proposed to eliminate the divergence of trajectory over time. The argument about attaching IMU to feet is that bipedal gait can split into two phases\u0026amp;#58 the stance phase and the swing phase. Under the assumption that the velocity is zero when the foot is stationary, it provides error-state Kalman Filter (ESKF), the drift corrector, pseudo-measurements to modify the position error.\nThe key to ZUPT with ESKF is precise zero velocity detection that distinguishes the stationary phase from each stride. Due to the complex differences in pedestrian motion patterns, the hope is that the zero velocity detector becomes robust for various individuals. While using other sensors besides IMU can realize precise detection, these systems tend to be intricate because of synchronizing each module. Utilizing the raw IMU readings only, we present a novel approach leveraging deep learning (DL) to detect zero velocity adaptively. Trained by massive foot-mounted IMU data from different individuals, the symmetrical Recurrent Convolutional Neural Network (RCNNs) can effectively learn the law of gait because the model takes the information from forward to backward of the undetermined time instant into consideration. The Convolutional Neural Network (CNN) and the Long Short-Term Memory (LSTM) in Symmetrical-Net extracts features and contextual information from input sequences, respectively. The proposed adaptive approach, unlike the conventional fixed threshold detector, requires no tuning on the optimal threshold, which will significantly elevate the applicability of ZUPT-aided INS. In summary, the main contributions of this project are as follows\u0026amp;#58\n Robust and adaptive zero velocity detection\u0026amp;#58 Leveraging the neural network, more robust and adaptive zero velocity detection is realized compared with the fixed threshold. The symmetrical framework also guarantees precise detection regardless of motion types and individual differences. Accurate and robust pedestrian tracking\u0026amp;#58 Performance of zero velocity detector makes a profound impact on the trajectory reconstruction of the ZUPT-aided INS system. The navigation system utilizing our proposed zero velocity detection shows significant improvement in terms of tracking accuracy and robustness. Extensive real-world validation\u0026amp;#58 Huge numbers of experiments reveal that INS assisted by Symmetrical-Net keep the high tracking accuracy in multiple indoor and outdoor environments under three paces (i.e., walking, fast walking, and running). Outstanding improvements, especially in robustness, on zero velocity detection via the proposed model are corroborated.  ","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"5302252e8f3539312931c7276cda35e3","permalink":"/project/project2/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/project/project2/","section":"project","summary":"Adaptive zero velocity detection for ZUPT-aided pedestrian navigation system.","tags":["Pedestrian Positioning"],"title":"Zero Velocity Detection","type":"project"},{"authors":["Ran Zhu, Zhuoling Xiao, Ying Li, Mingkun Yang, Yawen Tan, Liang Zhou, Shuisheng Lin, Hongkai Wen"],"categories":null,"content":"","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"9cca770e21ad764557fa2f543c1f0ea8","permalink":"/publication/project2/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/publication/project2/","section":"publication","summary":"The proliferation of smartphones has significantly facilitated people’s life, and diverse and powerful embedded sensors make the smartphone a ubiquitous platform to acquire and analyze data, which also provides great potential for efficient human activity recognition. We propose a learning-based ensemble model to improve the recognition accuracy of human activities, especially those that are easily confused. In order to make the model more robust and generalized, a huge amount of motion data, including 100 participants aging from 12 to 51, is collected using ordinary smartphones at a sampling rate of 50Hz. This dataset contains 7 motion modes in 4 smartphone placements under 2 different collection platforms (IOS and Android).","tags":["Source Themes"],"title":"Efficient Human Activity Recognition Solving the Confusing Activities via Deep Ensemble Learning","type":"publication"},{"authors":null,"categories":null,"content":"Human activity recognition (HAR) aiming to identify the actions carried out by a person given a set of observations of subject, has attracted much attention from both academia and industry with widely application requirements appearing in the indoor pedestrian tracking, healthcare, and smart cities. Currently, HAR methods can be mainly summarized as two categories: vision-based and sensor-based. Vision-based mainly relies on various high-frame-rate video devices. External factors such as lighting condition, clothing color, and image background have a great impact on recognition accuracy. The sensor-based approach, by contrast, is more robust in complex environments, which makes the system convenient and portable. Also, it can identify confusing human activities with the mathematical model by directly measuring the motion from human activities without infringement of personal privacy.\nWith the advent of miniaturized sensors and powerful computing resources in smartphones, the concept of efficient and ubiquitous HAR on smartphones is ready to fulfill soon. Among recent studies focusing on smartphone-based HAR, most researchers chose waist as the position to carry smartphones. However, the requirement for rigid attachment and specified placement is incompatible with the way in which people use mobile devices. For example, over a period of a few minutes, a smartphone could be carried in the backpack and then shifted to a pocket, before being taken out and being used to send a text message. This may be one of the main reasons why it is so hard to conduct HAR using smartphone sensors.\nExisting studies of sensor-based activity recognition often rely on supervised machine learning approaches such as Hidden Markov Model (HMM) , K-Nearest-Neighbors (KNN) , eXtreme Gradient Boosting (XGBoost), Random Forest (RF) and Support Vector Machine (SVM) using motion data collected from various types and quantities of motion sensors placed in different parts of body. However, these approaches are limited to three aspects: Firstly, due to the diversity and complexity of human activities, handcrafted feature extraction requires experience and expertise of the field. For the same reason, some extracted features show excellent performance in recognizing some activities, but rather bad at others. Secondly, even for the same activity, the waveforms of motion sensors are quite different in different smartphone placements. This makes it difficult to recognize various different activities with high precision. Thirdly, because of the differences in behavioral habits, gender, and age, the movement patterns of different people vary greatly, which enhances the difficulty of dividing the boundaries of different activities. The recognition accuracy tends to be limited due to confusing activities which generate similar motion signals.\nRecent years have witnessed fast development and unparalleled performance in many areas (i.e. image recognition, natural language processing) of deep learning. There is a growing trend of discovering meaningful representations of raw data by Convolutional Neural Network (CNN). It has shown great performance in different domains for avoiding handcrafted features. Therefore, we present the ensemble framework based on CNN to recognize human activities. Without tiring data preprocessing and feature extraction and selection, we put raw data that is partitioned by the sliding window into our network. By fully mining the information carried by the signal, it can achieve more accurate recognition on the combination of arbitrary activities and devices placement.\nThis project presents a framework and performance analysis of smartphone-sensor based HAR. Sensor data from accelerometer, gyroscope and magnetometer were collected when participants performed some typical and daily human activities: going upstairs, going downstairs, running, walking, standing, bicycling and swinging. We then used the ensemble of CNN to recognize human activities, especially those easily confused. The experiments have demonstrated the improvement on recognition accuracy with the proposed approach. In summary, the key contributions are:\n  A novel approach based on the ensemble of CNN has been proposed to solve the confusion between highly similar activities such as going upstairs and walking, which outperforms the single CNN model and achieves 96.11% accuracy.\n  Based on the collected data, we compare our model with the commonly used classifiers. The fact proves that the approach proposed in this paper outperforms other existing models in feasibility and efficiency.\n  A huge amount of motion data including 235 977 data samples from various types of motion sensors and sports scenes with different participants and postures are collected to validate the effectiveness of the proposed method.\n  ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"9b74b6d80c74c11210b876d3c28bd186","permalink":"/project/project1/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/project/project1/","section":"project","summary":"An efficient deep Ensemble method to recognize confusing activities.","tags":["Deep Learning"],"title":"Human Activity Recognition","type":"project"},{"authors":[],"categories":[],"content":"Deep Ensemble Learning For HAR Using Smartphone Conference PDF | Journal PDF\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to HAR.","tags":[],"title":"Slides","type":"slides"}]