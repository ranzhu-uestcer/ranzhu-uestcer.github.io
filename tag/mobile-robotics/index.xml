<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mobile Robotics | Ran&#39;s homepage</title>
    <link>/tag/mobile-robotics/</link>
      <atom:link href="/tag/mobile-robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Mobile Robotics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Mobile Robotics</title>
      <link>/tag/mobile-robotics/</link>
    </image>
    
    <item>
      <title>Visual Odometry</title>
      <link>/project/project3/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/project/project3/</guid>
      <description>&lt;p&gt;From Unmanned Ground Vehicles (UGVs) to Micro Aerial Vehicles (MAVs), it is essential to know where autonomous robots are and to perceive the surrounding area. Global Positioning System (GPS) provides information about the position of the sensor in the world coordinate. However, a precise self-localization purely relying on the GPS is not sufficient for challenging environments like indoor scenarios and urban canyons. In this situation, a more precise measure or an alternative localization system is required in the real application for autonomous driving.&lt;/p&gt;
&lt;p&gt;The camera is a small, light-weighted sensor that provides rich information about the environment around the sensing platform. It can recover the ego-motion from image sequences by exploiting the consistency between consecutive frames. Therefore, The concept of Visual Simultaneous Localization And Mapping (V-SLAM) and Visual Odometry (VO) are proposed to solve the well-known problem of positioning, which estimates vehicles&#39; position relative to its start point. As an essential task in robotics and computer vision communities, VO has been widely applied to various applications, ranging from autonomous driving and space exploration to virtual and augmented reality. From the perspective of the camera used, the VO methods consist of two types: stereo VO and monocular VO. This work aims at investigating the monocular VO, for a single camera is cheaper, lighter, and more general than a stereo rig. Especially when the ratio of stereo baseline to depth is minimal, the stereo VO degenerates to the monocular one.&lt;/p&gt;
&lt;p&gt;Over the past thirty years, enormous work has been done to develop an accurate and robust VO system. The traditional VO algorithms can be divided into the feature-based method and the direct method. Feature-based methods typically consist of camera calibration, feature detection, feature matching, outlier rejection (e.g., RANSAC), motion estimation, scale estimation, and optimization (e.g., Bundle Adjustment). Unfortunately, how to detect appropriate features for recovering specific motions remains a challenging problem. Handcrafted feature descriptors such as SIFT, ORB  designed for general visual tasks, lack the response to motions. Instead, extra information that is guided by geometric prior such as planar structures and vanishing points, is used for camera pose estimation in specific environments, providing promising performance but limited generalization ability. Unlike feature-based methods, direct methods track the motion of the pixel and obtain pose prediction by minimizing the photometric error, so it is extremely vulnerable to light changes. Moreover, the absolute scale estimation in the traditional monocular VO must use some extra information (e,g., the height of the camera) or prior knowledge.&lt;/p&gt;
&lt;p&gt;The emerging Deep Learning (DL), a data-driven approach, has yielded impressive achievement in the computer vision. Rather than handcrafted features, DL that has the ability to extract deep features from the plain input, encodes the high-level priors to regress camera poses. Compared with traditional VO, learning-based VO has the advantage of low computation cost and no need for internal camera parameters. A few methods on DL have been proposed for camera motion recovery, such as DeepVO, ESP-VO, SfmLearner, and GeoNet. While achieving promising performances, they do not take into account the different responses of visual cues and the effect of pixels movement in different directions in the input image to the camera motion, thus may output trajectories with large error. For learning-based VO, it should focus more on geometric constraints than the &amp;ldquo;appearance&#39;&#39; information when harnessing Convolutional Neural Networks (CNNs) to extract features. Optical flow, as the representation of the geometric structure, has been proved useful for estimating Frame to Frame (F2F) ego-motion. Therefore, we take the optical flow as input to the proposed model.&lt;/p&gt;
&lt;p&gt;Guided by the previous considerations, we explore a novel strategy for performing visual ego-motion estimation in this work. Here, we extend the network into four branches focusing on pixels movement in different directions in the optical flow and then regress the global feature concatenated from the four outputs to obtain F2F motion estimation. In particular, features extracted by each branch have been distilled by using the attention mechanism to refine estimation. In this project, many quantitative and qualitative experiments in terms of precision, robustness, and computation speed are conducted. The results demonstrate that the proposed model outperforms many current monocular methods and provides a competitive performance against the classic stereo VO. In summary, our key contributions are as follows&amp;amp;#58&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Novel visual perception guiding ego-motion estimation&amp;amp;#58 By considering the four quadrants in optical flow and fusing the distilling module into each branch encoder, the learning-based DeepAVO model pays more attention to the visual cues that are effective for ego-motion estimation.&lt;/li&gt;
&lt;li&gt;Lightweight VO framework with enhanced tracking performance&amp;amp;#58 The proposed DeepAVO model framework yields more robust and accurate results compared with competing monocular VOs. The F2F VO calculation can be done within 12 ms, making it practical and valuable in real-world applications.&lt;/li&gt;
&lt;li&gt;Extensive fresh scenes validation&amp;amp;#58 The DeepAVO produces promising pose estimation and maintains high-precision tracking results on the 11-20 sequence of the KITTI dataset and the Malaga dataset. Outstanding improvements in the accuracy and robustness of VO are further demonstrated.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Loop Closure Detection</title>
      <link>/project/project4/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/project/project4/</guid>
      <description>&lt;p&gt;Visual simultaneous localization and mapping (vSLAM) that simultaneously recovers camera pose and scene structure from video, as one of the key autonomous positioning and navigation technologies in areas where GPS fails or cannot be covered, is gaining importance in robotic applications such as autonomous cars or unmanned aerial vehicles. Loop Closure Detection (LCD), considered one of the essential parts in the visual SLAM system, is designed to recognize previsited areas by an autonomous mobile robot according to the image information collected by the visual sensors during the moving so also known as visual place recognition. Accurate LCD methods offer precise pose estimation by introducing extra constraints to correct the trajectory drift over time, improving the system performance. However, there are still two common challenges&amp;amp;#58 1) the same place has different appearances at different times due to change of illumination and weather; 2) different scenes look similar for reasons such as sharing common objects. Therefore, an excellent LCD method needs to resolve these two problems to detect more correct loops.&lt;/p&gt;
&lt;p&gt;In the field of vSLAM, the appearance-based methods treating LCD as an image matching problem compare the similarity between the current image and previous images. If the similarity between them is sufficiently high to exceed a given threshold, we can regard it as a loop closure. As one of the most popular LCD approaches, it can be divided into two crucial steps: feature generation and similarity measurement.&lt;/p&gt;
&lt;p&gt;In the vSLAM system, image feature extraction forms the basis of a series of tasks, such as keyframe extraction, tracking, positioning, and map construction, which have a decisive influence on the robot’s autonomous positioning. The traditional appearance-based methods mainly follow the visual bag-of-words (BoWs) model, which uses a clustering procedure on a training sample of local features and quantizes the descriptor space into Visual Words (VWs). However, this approach uses hand-crafted traditional features. Most of these features discard certain geometric and structural information, making it difficult to cope with the challenges such as camera motion and illumination changes. Moreover, BoWs relies on specific environments and has poor robustness to different application scenarios. In recent years, deep learning has made significant breakthroughs in the field of computer vision. Much related research demonstrates that the deep features learned by convolutional neural networks (CNNs) can provide more robust image representations in changeable environmental conditions, especially when illumination change and viewpoint variation. Besides, the network models trained for specific tasks can be transferred to other tasks successfully. Some classic pre-training network models’ availability makes it more convenient to complete various feature extraction tasks. Based on the above reasons, researchers began to apply CNNs to LCD. Although good results have been achieved, these works paid little attention to similarity measurement strategies, which also play a crucial role in image matching tasks. Fixed pre-specified distance metrics such as Euclidean distance or cosine similarity are commonly used. Moreover, feature extraction exists independently of similarity measurement, its effectiveness severely restrains the similarity measurement’s effect. For these reasons, further improvement of detection precision is hindered.&lt;/p&gt;
&lt;p&gt;This project proposes a two-branch end-to-end network called MetricNet that performs LCD based on the adaptive weighted similarity matrix, jointly optimized with feature extraction, to address the issues mentioned earlier. In summary, the key contributions are as follows&amp;amp;#58&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adaptive feature selection and similarity matrix&amp;amp;#58 By using the channel weighting to remove the irrelevant background information and a weighted similarity matrix to adaptively select spatial information, MetricNet has been proved to be more robust and accurate than competitive models in both theory and practice.&lt;/li&gt;
&lt;li&gt;An end-to-end LCD framework&amp;amp;#58 this research also proposes a novel LCD framework that encompasses both feature extraction and similarity measurement to extract more effective representation. The learning-based MetricNet can achieve a recall rate up to 47.08% under 100 precision.&lt;/li&gt;
&lt;li&gt;Extensive multi-dataset validation&amp;amp;#58 MetricNet has proved its excellent performance on three typical open datasets under drastic illumination and seasonal variations. Outstanding improvements in detection accuracy and generalization ability are also further demonstrated.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
